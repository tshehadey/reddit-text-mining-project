{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db718591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\"\"\"For topic modeling, we used the same models used in the book\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d7bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and preprocessed data\n",
    "df = pd.read_csv('../data/reddit_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ad8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5670a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating count-vectorizer for pre-processed data\n",
    "count_text_vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53018ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TF-IDF vectorizier for pre-processed data\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c27842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  study (2.58)\n",
      "  new (2.43)\n",
      "  people (1.22)\n",
      "  use (1.17)\n",
      "  risk (0.91)\n",
      "\n",
      "Topic 01\n",
      "  trump (3.26)\n",
      "  tech (3.22)\n",
      "  china (2.00)\n",
      "  power (1.36)\n",
      "  job (1.22)\n"
     ]
    }
   ],
   "source": [
    "# Nonnegative Matrix Factorization (NMF) Model\n",
    "nmf_text_model = NMF(n_components=2, random_state=509)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())\n",
    "\n",
    "\"\"\"Topic 00 is the r/science and topic 01 is r/technology if we compare these results to the word clouds we showed earlier\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  new (2.09)\n",
      "  study (2.01)\n",
      "  use (1.20)\n",
      "  people (0.96)\n",
      "  risk (0.71)\n",
      "\n",
      "Topic 01\n",
      "  trump (6.90)\n",
      "  tech (6.80)\n",
      "  china (4.53)\n",
      "  power (2.80)\n",
      "  google (2.51)\n"
     ]
    }
   ],
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "lsa_text_model = TruncatedSVD(n_components=2, random_state=509)\n",
    "W_svd_para_matrix = lsa_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_para_matrix = lsa_text_model.components_\n",
    "\n",
    "display_topics(lsa_text_model, tfidf_text_vectorizer.get_feature_names_out())\n",
    "\n",
    "\"\"\"LSA gives us similar results to our previous model used\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  study (2.47)\n",
      "  new (2.24)\n",
      "  people (1.57)\n",
      "  use (1.33)\n",
      "  risk (1.17)\n",
      "\n",
      "Topic 01\n",
      "  new (1.72)\n",
      "  study (1.35)\n",
      "  trump (1.19)\n",
      "  scientist (0.95)\n",
      "  researcher (0.93)\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation (LDA) model\n",
    "lda_text_model = LatentDirichletAllocation(n_components = 2, random_state=509)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "display_topics(lda_text_model, tfidf_text_vectorizer.get_feature_names_out())\n",
    "\n",
    "\"\"\"LDA produced different outputs for the topic 01, we can see scientist and researcher are included in the results as one of the top topics\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165b112",
   "metadata": {},
   "source": [
    "Three topic models were built to study the different topics within the text from the science subreddit and the technology subreddit.\n",
    "\n",
    "The Non-negative matrix factorization (NMF) model on the left includes words like “study”, “people”, and “risk” in topic 00, and words such as “trump”, “tech”, and “power” for topic 01. Based on the original categories of the data, Topic 00 would seem to match the science subreddit, and Topic 01 would seem to match the technology subreddit.\n",
    "\n",
    "Next, a Latent Semantic Analysis (LSA) model was built. This model included the same words to the NMF model, but with different percentages of contribution to their topic. Topic 01 for the LSA model also has the same words as the NMF model, and with the same order of percentage of contribution. However, the percentages differ between the two models.\n",
    "\n",
    "Finally, a Latent Dirichlet Allocation (LDA) model was built. This model included the same words as the previous two models for Topic 00. The LDA and NMF model also share the same order of percentage of contribution. The LDA model’s Topic 01 differs from the previous two models in that it includes the words “researcher”, “scientist”, and “new”. The word “new” was already mentioned for the LDA Topic 00."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
