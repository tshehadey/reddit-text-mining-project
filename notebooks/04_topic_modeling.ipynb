{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db718591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d7bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and preprocessed data\n",
    "df = pd.read_csv('../data/reddit_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ad8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5670a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53018ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c27842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  ai (13.91)\n",
      "  job (2.43)\n",
      "  model (2.19)\n",
      "  use (1.74)\n",
      "  google (1.69)\n",
      "\n",
      "Topic 01\n",
      "  study (2.27)\n",
      "  new (2.18)\n",
      "  people (1.10)\n",
      "  use (0.98)\n",
      "  risk (0.81)\n"
     ]
    }
   ],
   "source": [
    "nmf_text_model = NMF(n_components=2, random_state=509)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  ai (3.94)\n",
      "  new (1.86)\n",
      "  study (1.58)\n",
      "  use (1.31)\n",
      "  model (0.84)\n",
      "\n",
      "Topic 01\n",
      "  ai (6.75)\n",
      "  job (1.22)\n",
      "  google (0.74)\n",
      "  ceo (0.66)\n",
      "  model (0.63)\n"
     ]
    }
   ],
   "source": [
    "lsa_text_model = TruncatedSVD(n_components=2, random_state=509)\n",
    "W_svd_para_matrix = lsa_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_para_matrix = lsa_text_model.components_\n",
    "\n",
    "display_topics(lsa_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cc577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  ai (1.68)\n",
      "  new (1.59)\n",
      "  trump (1.32)\n",
      "  use (1.11)\n",
      "  tech (0.97)\n",
      "\n",
      "Topic 01\n",
      "  study (3.14)\n",
      "  new (2.20)\n",
      "  ai (1.85)\n",
      "  people (1.39)\n",
      "  risk (1.04)\n"
     ]
    }
   ],
   "source": [
    "lda_text_model = LatentDirichletAllocation(n_components = 2, random_state=509)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "display_topics(lda_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
