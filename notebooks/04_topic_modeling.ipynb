{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db718591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d7bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and preprocessed data\n",
    "df = pd.read_csv('/Users/mtc/ADS/ADS 509/reddit-text-mining-project/data/reddit_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ad8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5670a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53018ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c27842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  study (2.42)\n",
      "  new (2.11)\n",
      "  find (2.04)\n",
      "  people (1.09)\n",
      "  use (0.93)\n",
      "\n",
      "Topic 01\n",
      "  ai (13.15)\n",
      "  job (2.32)\n",
      "  model (2.08)\n",
      "  use (1.69)\n",
      "  google (1.61)\n"
     ]
    }
   ],
   "source": [
    "nmf_text_model = NMF(n_components=2, random_state=509)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  ai (3.34)\n",
      "  new (1.84)\n",
      "  study (1.73)\n",
      "  find (1.43)\n",
      "  use (1.22)\n",
      "\n",
      "Topic 01\n",
      "  ai (10.54)\n",
      "  job (1.94)\n",
      "  google (1.22)\n",
      "  model (1.15)\n",
      "  ceo (1.03)\n"
     ]
    }
   ],
   "source": [
    "lsa_text_model = TruncatedSVD(n_components=2, random_state=509)\n",
    "W_svd_para_matrix = lsa_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_para_matrix = lsa_text_model.components_\n",
    "\n",
    "display_topics(lsa_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1cc577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  ai (4.05)\n",
      "  new (1.82)\n",
      "  trump (1.20)\n",
      "  use (1.11)\n",
      "  tech (0.88)\n",
      "\n",
      "Topic 01\n",
      "  study (3.21)\n",
      "  find (2.45)\n",
      "  new (1.99)\n",
      "  people (1.44)\n",
      "  risk (1.08)\n"
     ]
    }
   ],
   "source": [
    "lda_text_model = LatentDirichletAllocation(n_components = 2, random_state=509)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "display_topics(lda_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
