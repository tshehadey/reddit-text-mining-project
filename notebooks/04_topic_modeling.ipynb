{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db718591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d7bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and preprocessed data\n",
    "df = pd.read_csv('../data/reddit_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ad8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5670a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating count-vectorizer for pre-processed data\n",
    "count_text_vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53018ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TF-IDF vectorizier for pre-processed data\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c27842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  study (2.58)\n",
      "  new (2.43)\n",
      "  people (1.22)\n",
      "  use (1.17)\n",
      "  risk (0.91)\n",
      "\n",
      "Topic 01\n",
      "  trump (3.26)\n",
      "  tech (3.22)\n",
      "  china (2.00)\n",
      "  power (1.36)\n",
      "  job (1.22)\n"
     ]
    }
   ],
   "source": [
    "# Nonnegative Matrix Factorization (NMF) Model\n",
    "nmf_text_model = NMF(n_components=2, random_state=509)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  new (2.09)\n",
      "  study (2.01)\n",
      "  use (1.20)\n",
      "  people (0.96)\n",
      "  risk (0.71)\n",
      "\n",
      "Topic 01\n",
      "  trump (6.90)\n",
      "  tech (6.80)\n",
      "  china (4.53)\n",
      "  power (2.80)\n",
      "  google (2.51)\n"
     ]
    }
   ],
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "lsa_text_model = TruncatedSVD(n_components=2, random_state=509)\n",
    "W_svd_para_matrix = lsa_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_para_matrix = lsa_text_model.components_\n",
    "\n",
    "display_topics(lsa_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cc577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  study (2.47)\n",
      "  new (2.24)\n",
      "  people (1.57)\n",
      "  use (1.33)\n",
      "  risk (1.17)\n",
      "\n",
      "Topic 01\n",
      "  new (1.72)\n",
      "  study (1.35)\n",
      "  trump (1.19)\n",
      "  scientist (0.95)\n",
      "  researcher (0.93)\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation (LDA) model\n",
    "lda_text_model = LatentDirichletAllocation(n_components = 2, random_state=509)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "display_topics(lda_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
